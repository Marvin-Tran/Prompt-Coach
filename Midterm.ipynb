{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e31e2b5-3b82-46b7-846a-527571a9a073",
   "metadata": {},
   "source": [
    "# What impact does emotional tone (anger, sadness, and anxiety) in prompts have on the Large Language Model responses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8b0dd-1482-4b60-8899-a311f9488f0b",
   "metadata": {},
   "source": [
    "Which emotion has the highest impact on [metric] in LLM responses? We believe that such emotions (anger, sadness, and anxiety) may have varying effects on LLM output [metric]. For example, prompts with high anxiety may produce 'better' prompts because the model could sense the urgency in the situation.\n",
    "\n",
    "How can we do this? We must curate public datasets of real conversations between users and LLMs. In this project we will primarily be working with WildChat and ShareGPT52k, which hold around 1 million conversations.\n",
    "\n",
    "To analyze emotional tone in prompts, we utilize a proprietary service, LIWC, that calculates the percentage of words in a sentence that relates to specific categories. To measure Large Language Model responses, we will either use BigBench or HELM, whichever we get to work. Additionally, we use matplotlib and seaborn to visualize our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0726f37-6b69-4a6d-8415-064b8e70e05e",
   "metadata": {},
   "source": [
    "In step 1 we load the data from HuggingFace and filter them based on keywords we believe pertain to the outputs we're measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "452674c9-5542-433d-b136-675e77df8072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "from datasets import load_dataset\n",
    "wildchat = load_dataset(\"allenai/WildChat-1M\", split='train')\n",
    "sharegpt = load_dataset('RyokoAI/ShareGPT52K', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1735a-c308-4236-9e50-56d508a81d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List keywords for filtering\n",
    "keywords = [\n",
    "    \"race\", \"ethnicity\", \"gender\", \"woman\", \"man\", \"nonbinary\", \"trans\",\n",
    "    \"black\", \"white\", \"asian\", \"latino\", \"lgbt\", \"queer\", \"gay\", \"lesbian\",\n",
    "    \"stereotype\", \"bias\", \"prejudice\", \"minority\", \"discrimination\",\n",
    "    \"religion\", \"muslim\", \"jewish\", \"christian\", \"age\", \"old\", \"young\", \"elderly\",\n",
    "    \"ableist\", \"disabled\", \"mental health\", \"autism\", \"fat\", \"body image\",\n",
    "    \"look like\", \"appearance\", \"skin color\", \"accent\"\n",
    "]\n",
    "\n",
    "game_keywords = [\n",
    "    'game', 'video game', 'playstation', 'xbox', 'nintendo', 'minecraft', 'fortnite',\n",
    "    'roblox', 'gta', 'call of duty', 'zelda', 'pokemon', 'league of legends', 'valorant',\n",
    "    'esports', 'gamer', 'console', 'controller', 'high score', 'multiplayer', 'fps',\n",
    "    'rpg', 'tournament', 'speedrun', 'gaming setup', 'streamer', 'twitch', 'steam',\n",
    "    'mod', 'boss fight', 'quest', 'level up', 'open world', 'battle royale', 'skins',\n",
    "    'beat the boss', 'best game', 'favorite character', 'choose your fighter', 'superhero'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d1e84-dada-4bb8-8f1f-b1be907e7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wildchat_convo = []\n",
    "\n",
    "for conversation in wildchat['conversation']:\n",
    "    for turn in range(0, len(conversation) - 1, 2):\n",
    "        user_turn = conversation[turn]\n",
    "        assistant_turn = conversation[turn + 1]\n",
    "\n",
    "        # Ensure both parts of the exchange are in English\n",
    "        if user_turn.get('language') == \"English\" and assistant_turn.get('language') == \"English\":\n",
    "            prompt = user_turn.get('content', '').strip().lower()\n",
    "            response = assistant_turn.get('content', '').strip()\n",
    "\n",
    "            # Check conditions\n",
    "            has_demo_kw = any(kw in prompt for kw in keywords)\n",
    "            has_no_game_kw = not any(gk in prompt for gk in game_keywords)\n",
    "            is_long_enough = len(prompt.split()) >= 5\n",
    "\n",
    "            if has_demo_kw and has_no_game_kw and is_long_enough:\n",
    "                wildchat_convo.append({\n",
    "                    'prompt': user_turn['content'],\n",
    "                    'response': response\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82557bf8-584d-4bd7-9c61-5f69717fca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a913f7-e75d-46ec-aca7-056b74f18931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langdetect import detect\n",
    "\n",
    "sharegpt_convo = []\n",
    "\n",
    "# Helper functions\n",
    "def contains_whole_word(text, keywords):\n",
    "    return any(re.search(rf\"\\b{k}\\b\", text) for k in keywords)\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "for example in sharegpt:\n",
    "    messages = example.get(\"conversations\", [])\n",
    "\n",
    "    if not isinstance(messages, list) or not all(isinstance(m, dict) for m in messages):\n",
    "        continue\n",
    "\n",
    "    for i in range(0, len(messages) - 1, 2):\n",
    "        user = messagewwws[i]\n",
    "        bot = messages[i + 1]\n",
    "\n",
    "        prompt = user.get(\"value\", \"\").lower()\n",
    "        response = bot.get(\"value\", \"\")\n",
    "\n",
    "        if (\n",
    "            contains_whole_word(prompt, keywords)\n",
    "            and not contains_whole_word(prompt, game_keywords)\n",
    "            and is_english(prompt)\n",
    "            and is_english(response)\n",
    "        ):\n",
    "            sharegpt_convo.append({\"prompt\": prompt, \"response\": response})\n",
    "            \n",
    "    # Cap results for testing\n",
    "    if len(sharegpt_convo) >= 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181e045-59ca-4fa3-abcf-58e63a5cc1f0",
   "metadata": {},
   "source": [
    "In step 2 we convert our data into Pandas Dataframes and export them for LIWC analysis. Then import them back for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1a9a4-76a7-4fb4-b893-2cf3304d75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert into Pandas DataFrames\n",
    "wildchat_convo = pd.DataFrame(wildchat_convo, columns=['prompt','response'])\n",
    "sharegpt_convo = pd.DataFrame(sharegpt_convo, columns=['prompt','response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a528277-4d93-4a2f-8697-32302aa18d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for the next step\n",
    "wildchat_convo.to_csv('wildchat_convo.csv', index=False)\n",
    "sharegpt_convo.to_csv('sharegpt_convo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898595c5-3cfb-430b-918d-17565d32d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LIWC Analysis\n",
    "# wildchat_liwc = pd.read_csv('LIWC-22 Results - wildchat_convo - LIWC Analysis.csv')\n",
    "sharegpt_liwc = pd.read_csv('LIWC-22 Results - sharegpt_convo - LIWC Analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa69f4f-5dcf-4cf8-8054-3f2adae50d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wildchat_liwc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4957a-70a3-40eb-953d-f2692a29c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharegpt_liwc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb64ec-0e29-414d-84a2-1713dd9d6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select LIWC categories\n",
    "liwc_categories = ['emo_pos', 'emo_neg', 'emo_anger', 'emo_anx', 'emo_sad', 'swear']\n",
    "\n",
    "# Subset and reshape to long format\n",
    "wildchat_long = wildchat_liwc[liwc_categories].melt(var_name='Category', value_name='Score')\n",
    "sharegpt_long = sharegpt_liwc[liwc_categories].melt(var_name='Category', value_name='Score')\n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Category', y='Score', data=wildchat_long)\n",
    "plt.title(\"WildChat Distribution of LIWC Category Scores\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Category', y='Score', data=sharegpt_long)\n",
    "plt.title(\"ShareGPT52K Distribution of LIWC Category Scores\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "wildchat_liwc[liwc_categories].mean().sort_values(ascending=False).plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 6),\n",
    "    title=\"Mean WildChat LIWC Category Scores Across Prompts\"\n",
    ")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sharegpt_liwc[liwc_categories].mean().sort_values(ascending=False).plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 6),\n",
    "    title=\"Mean ShareGPT52K LIWC Category Scores Across Prompts\"\n",
    ")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(wildchat_liwc[liwc_categories].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Between WildChat LIWC Categories\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sharegpt_liwc[liwc_categories].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Between ShareGPT52K LIWC Categories\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21021af1-e137-440b-bc09-14116288d8a9",
   "metadata": {},
   "source": [
    "Based on our bar chat, positive and negative emotions show a higher prevalency that the sub emotions of anger, sadness, and anxiety. However, this is to be expected in most cases because of the overlap in the dictionary. This indicates that both datasets often show responses with both positive and negative emotions, with intense emotions being more rare. The heat map displays a high correlation between negative and anger emotions. Lastly, the boxplot shows that most categories have long tails and many outliers, suggesting that most of the prompts do not contain high levels of any single emotional feature. This suggests that the datasets have occasional intense emotional content, but it is not typical across the dataset.\n",
    "\n",
    "Our next steps in this project are to select the few outliers present and adjust their emotional intensity using Groq and Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55a173-3417-40c3-80a0-f4e0820251fe",
   "metadata": {},
   "source": [
    "In step 1 we further curate a subset of prompts from our Data Frames for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5eeed-b7e9-414e-831f-56ddca0de3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in sharegpt_convo['prompt']:\n",
    "    print(prompt)\n",
    "    print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0358c26-4f57-4b85-aa3d-f940e991bf2b",
   "metadata": {},
   "source": [
    "In step 2 we define a method using LangChain that modifies and returns a prompt into 5 different emotional intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d7e73-084c-46da-ab99-4c0be7dd26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain langchain-groq  langchain-core\n",
    "\n",
    "GROQ_API_KEY=\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(temperature=2, groq_api_key=GROQ_API_KEY, model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d77bcb-7d43-4031-bb8f-dc00379c3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are an assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "def emo_gen(text, emotion):\n",
    "    chain = prompt | chat\n",
    "    response = chain.invoke({\n",
    "        \"text\": f\"<I want to feed prompts into an LM. Provide me 5 different versions of the following prompt with varying '{emotion}' levels, 1 being not '{emotion}' and 5 being extremely '{emotion}' (Please just give the text no other information): '{text}'>\"\n",
    "    })\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c3315-f497-49e4-b6eb-500be858fd36",
   "metadata": {},
   "source": [
    "In step 3 we apply our new method to the curated prompts and collect them into a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17df77-f18b-4ba0-b4a7-0ee855399db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "pattern = r\"(?m)^(\\d+)\\.\\s+(.*)\"\n",
    "\n",
    "anger_prompts = defaultdict(list)\n",
    "sad_prompts = defaultdict(list)\n",
    "anxious_prompts = defaultdict(list)\n",
    "\n",
    "curated_prompts = [\"Why are some people bad at driving?\", \"Why do asians make alot of noise\"]\n",
    "\n",
    "for curated_prompt in curated_prompts:\n",
    "    \n",
    "    anger_response = emo_gen(curated_prompt, \"anger\")\n",
    "    sad_response = emo_gen(curated_prompt, \"sadness\")\n",
    "    anxious_response = emo_gen(curated_prompt, \"anxious\")\n",
    "    \n",
    "    anger_matches = re.findall(pattern, anger_response)\n",
    "    for degree, sentence in anger_matches:\n",
    "        anger_prompts[degree].append(sentence)\n",
    "    sad_matches = re.findall(pattern, sad_response)\n",
    "    for degree, sentence in sad_matches:\n",
    "        sad_prompts[degree].append(sentence)\n",
    "    anxious_matches = re.findall(pattern, anxious_response)\n",
    "    for degree, sentence in anxious_matches:\n",
    "        anxious_prompts[degree].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10971ec2-f81f-4431-9409-3d05e7af18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482551ec-f38c-4f7b-82bb-f05d368e5f42",
   "metadata": {},
   "source": [
    "In step 4 we run our new prompts into an LLM and collect their outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e63627-1eb5-4714-bc29-5c411ef37ce0",
   "metadata": {},
   "source": [
    "In step 5 we measure the outputs using BigBench."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1faebe-e4d1-4f74-80e9-1450a23ee5b2",
   "metadata": {},
   "source": [
    "In step 6 we conduct correlation analysis and visualize our results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
